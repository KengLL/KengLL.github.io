---
layout: post
title: Image Inpainting
github: "https://github.com/hlm01/image_inpainting"
image: "/img/in-project/inpaint.png"
tech:
  - name: "Pytorch"
    icon: "/img/tech/pytorch.png"
  - name: "Python"
    icon: "/img/tech/python.png"
description: Implement Partial Convolution for free form inpaining
tags:
  - "AI/ML"
  - "Research"
---

# Image Inpainting with Partial Convolutions: Making Holes Disappear

_By Kengâ€‘Lien Lin & Henry Lin_

## ğŸŒŸ Project Overview

In our ECEâ€¯176 final project, we implemented a deep learningâ€“based **image inpainting model** inspired by Liu etâ€¯al. (2018). This model tackles the challenge of reconstructing missing parts of an imageâ€”especially when the holes are irregularly shaped.

We trained our model on human face images from the **Flickrâ€‘Facesâ€‘HQ (FFHQ)** dataset, using **partial convolution layers** within a modified Uâ€‘Net architecture. These layers skip over missing regions during convolution, enabling more realistic and seamless image completion.

> **Goal**: Fill in irregular holes in face images using a network that learns both global semantics and local details.

---

## ğŸ” Motivation

Traditional patchâ€‘based inpainting works for small, patterned holesâ€”but struggles with larger or irregularly shaped missing regions. Recent deep learning models often handle only **rectangular masks**, leading to visible artifacts around nonâ€‘rectangular holes.

**Partial convolutions** are maskingâ€‘aware operations that only consider valid (nonâ€‘hole) pixels during convolution and normalize outputs accordingly, reducing shapeâ€‘dependent artifacts.

---

## ğŸ“š Related Work

- **Patchâ€‘based inpainting** (Telea,â€¯2004): Effective for textures, but limited on complex scenes.  
- **Uâ€‘Net architecture** (Ronneberger etâ€¯al.,â€¯2015): Encoderâ€“decoder with skip connections for detail preservation.  
- **Contextual attention + gated convolutions** (Yu etâ€¯al.,â€¯2018): Improved refinement at the cost of higher memory usage.  

![U_Net_Structure](/img/in-project/Group14.jpg)
---

## ğŸ§  Partial Convolution Primer

A partial convolution at each location only processes pixels marked valid by a binary mask \(M\):

$$
x' = 
\begin{cases}
\dfrac{W^T (X \odot M)}{\sum(M)} + b, & \text{if }\sum(M) > 0,\\[1ex]
0, & \text{otherwise},
\end{cases}
$$

- \(X\): input patch  
- \(M\): binary mask (1 = known, 0 = hole)  
- \(W\): convolution kernel  
- \(b\): bias term  

After each layer, the mask is updated so that any location with \(\ge1\) valid pixel becomes valid in deeper layers.

![PartialConv](/img/in-project/slide_6.jpg)
---

## ğŸ—ï¸ Network Architecture

We adapted a **Uâ€‘Net** model with:

- Partial convolution layers replacing standard convolutions  
- Encoder: ReLU + BatchNorm  
- Decoder: LeakyReLU (\(\alpha = 0.2\)) + BatchNorm  
- Nearestâ€‘neighbor upsampling  
- One fewer downâ€‘/upâ€‘sampling stage (to suit 256Ã—256 images)

![Architecture Diagram](path/to/unet_partial_conv.png)

---

## âš–ï¸ Loss Function

A multiâ€‘term loss ensures both pixel accuracy and perceptual quality:

- $\(L_{\text{valid}}\)$: perâ€‘pixel loss on known regions  
- $\(L_{\text{hole}}\)$: perâ€‘pixel loss on inpainted regions  
- $\(L_{\text{perceptual}}\)$: featureâ€‘space difference via VGGâ€‘16  
- $\(L_{\text{style\_out}}\) \& \(L_{\text{style\_comp}}\)$: style consistency via Gram matrices  
- $\(L_{\text{tv}}\)$**$: total variation loss for smoothness  

$$
L_{\text{total}} = 
L_{\text{valid}}
+ 6\,L_{\text{hole}}
+ 0.05\,L_{\text{perceptual}}
+ 120\,(L_{\text{style_out}} + L_{\text{style_comp}})
+ 0.1\,L_{\text{tv}}
$$

---

## ğŸƒ Training Setup

- **Dataset**: FFHQ (52â€¯k face images at 256Ã—256)  
- **Masks**: 1â€¯000 freeâ€‘form binary masks  
- **Hardware**: Nvidia RTXâ€¯2060  
- **Batch Size**: 12  
- **Optimizer**: Adam (LR = 0.0002)  
- **Training Time**: ~2â€¯days â†’ 35â€¯epochs (â‰ˆ10% of original training)

---

## ğŸ“Š Experiments & Results

### CaseÂ 1: Sparse Freeâ€‘form Masks

Model blends the hole with surroundings and recovers details (e.g., glasses, facial features).

> Artifacts (rainbow specks, color blobs) appear in dense regions but decrease with more training.

### CaseÂ 2: Large Central Masks

Even with minimal input, the model learns face structure and symmetry (e.g., beard extension).

> Borders are more pronounced; color matching is less accurate.

### CaseÂ 3: Comparison with Fully Trained Models

Compared to Liu et al.â€™s PartialConv and GntIpt models, our partially trained network shows more artifacts and color mismatches.

---

## ğŸ§© Key Insights

- **Partial convolutions** effectively reduce maskâ€‘shape artifacts.  
- Even limited training yields **semantic understanding** of faces.  
- **Longer training** and fineâ€‘tuning should significantly improve output quality.

---

## ğŸ’¡ Conclusion

Our implementation of **maskingâ€‘aware partial convolutions** demonstrates robust freeâ€‘form image inpainting capabilities. With additional training and hyperparameter tuning, this approach promises strong performance for realâ€‘world image editing and restoration tasks.

---

## ğŸ“¸ Acknowledgments

- **Dataset**: Flickrâ€‘Facesâ€‘HQ (Karras etâ€¯al.,â€¯2019)  
- **Core Paper**: Liu etâ€¯al.,â€¯2018, â€œImage Inpainting for Irregular Holes using Partial Convolutionsâ€  
- **Uâ€‘Net Reference**: Ronneberger etâ€¯al.,â€¯2015  

